<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Chengzhi Mao — Rutgers CS</title>
  <meta name="description" content="Chengzhi Mao — Assistant Professor, Rutgers CS. Research in LLMs, Computer Vision, and Robust ML." />
  <style>
    :root{--max:980px;--fg:#111;--muted:#666;--link:#0a58ca;--bg:#fff;--border:#eee}
    *{box-sizing:border-box}
    html,body{margin:0;padding:0;background:var(--bg);color:var(--fg);font:16px/1.6 system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial,sans-serif}
    a{color:var(--link);text-decoration:none} a:hover{text-decoration:underline}
    .wrap{max-width:var(--max);margin:40px auto;padding:0 20px}
    header{display:grid;grid-template-columns:1fr 220px;gap:24px;align-items:start}
    h1{font-size:2rem;margin:.25rem 0}
    .affil{white-space:pre-line;color:var(--muted)}
    .contact a{margin-right:10px}
    .headshot{width:220px;height:220px;object-fit:cover;border-radius:8px;border:1px solid var(--border)}
    nav{margin:16px 0 24px 0} nav a{margin-right:14px;color:#444} nav a:hover{color:var(--fg)}
    section{padding:18px 0;border-top:1px solid var(--border)}
    h2{font-size:1.25rem;margin:0 0 .5rem 0}
    .news li{margin:.2rem 0}
    .grid{display:grid;grid-template-columns:1fr 1fr;gap:24px}
    .paper{display:grid;grid-template-columns:110px 1fr;gap:12px;align-items:start}
    .thumb{width:110px;height:78px;object-fit:cover;border:1px solid var(--border);border-radius:4px;background:#fafafa}
    .paper h3{font-size:1rem;margin:0}
    .meta{font-size:.95rem;color:var(--muted)}
    .by{font-size:.95rem}
    .year{margin-top:8px;font-weight:600}
    ul{padding-left:18px;margin:8px 0}
    .teach li{margin:.2rem 0}
    .students{display:grid;grid-template-columns:repeat(auto-fill,minmax(180px,1fr));gap:12px}
    .badge{display:block;padding:10px;border:1px solid var(--border);border-radius:6px}
    footer{color:var(--muted);font-size:.92rem;margin:24px 0 40px}
    @media (max-width:800px){
      header{grid-template-columns:1fr}
      .headshot{justify-self:start}
      .grid{grid-template-columns:1fr}
      .paper{grid-template-columns:110px 1fr}
    }
  </style>
</head>
<body>
  <div class="wrap">
    <header>
      <div>
        <h1>Chengzhi Mao</h1>
        <div class="affil">
Assistant Professor, Department of Computer Science, Rutgers University
Research Scientist, Google (LLMs, Vision, Robust ML)
        </div>
        <div class="contact" style="margin-top:8px">
          <a href="mailto:cm1838@scarletmail.rutgers.edu">cm1838@scarletmail.rutgers.edu</a>
          <a href="https://scholar.google.com/citations?user=">Google Scholar</a>
          <a href="https://selfie.cs.columbia.edu">SelfIE</a>
          <a href="assets/docs/Chengzhi_Mao_CV.pdf">CV (PDF)</a>
        </div>
        <nav>
          <a href="#bio">Bio</a>
          <a href="#students">Students</a>
          <a href="#research">Papers</a>
          <a href="#teaching">Teaching</a>
          <a href="#contact">Contact</a>
        </nav>
      </div>
      <img class="headshot" src="assets/img/Chengzhi.jpeg" alt="Chengzhi Mao headshot" />
    </header>

    <section id="bio">
      <h2>Brief Bio</h2>
      <p>
        I am an Assistant Professor of Computer Science at Rutgers University and a Research Scientist at Google.
        My research spans large language models, computer vision, and robust machine learning—especially how to make
        foundation models reliable in open-world, nonstationary settings. Previously, I was a Core Academic Member at
        MILA and an Assistant Professor at McGill. I completed my Ph.D. at Columbia University, advised by
        Junfeng Yang and Carl Vondrick.
      </p>
      <p>
        I am actively recruiting highly self-motivated Ph.D. students and postdocs. If you’re interested, please email me with your CV and a brief note on how your background aligns with robust and causal learning for LLMs and vision.
      </p>
    </section>

    <section id="news">
      <h2>News</h2>
      <ul class="news">
        <li><strong>Oct 2025:</strong> Giving an invited talk at ICCV 2025.</li>
        <li><strong>Sep 2025:</strong> Two papers accepted to NeurIPS 2025.</li>
        <li><strong>Jan 2025:</strong> One paper accepted to ICLR 2025 and one to NAACL 2025.</li>
        <li><strong>Nov 2024:</strong> Paper accepted to ASPLOS 2025 and EMNLP 2025 (Main).</li>
        <li><strong>Oct 2024:</strong> Serving as Area Chair for ICLR 2025.</li>
        <li><strong>Apr 2024:</strong> ICML 2024 paper on LLM interpretation accepted.</li>
        <li><strong>Feb 2024:</strong> CVPR 2024 Highlight (top 2%).</li>
      </ul>
    </section>

    <section id="students">
      <h2>PhD Students and Postdocs</h2>
      <div class="students">
        <span class="badge">Yang Li — PhD Student</span>
        <span class="badge">Zirui Zhang — PhD Student</span>
      </div>
    </section>

    <section id="research">
      <h2>Selected Papers</h2>

      <!-- 2025 -->
      <div class="year">2025</div>
      <div class="grid">
        <div class="paper">
          <img class="thumb" src="assets/img/papers/textexplain.gif" alt="NeurIPS 2025 thumbnail" />
          <div>
            <h3>Latent Adversarial Reflection for LLM Jailbreaking (LARGO)</h3>
            <div class="by"><strong>Chengzhi Mao</strong>, et al.</div>
            <div class="meta">NeurIPS 2025 — <a href="https://arxiv.org/abs/2505.10838">paper</a></div>
          </div>
        </div>
      </div>

      <!-- 2024–2025 -->
      <div class="year">2024–2025</div>
      <div class="grid">
        <div class="paper">
          <img class="thumb" src="assets/img/papers/selfie.svg" alt="SelfIE thumbnail" />
          <div>
            <h3>SelfIE: Self-Interpretation of Large Language Model Embeddings</h3>
            <div class="by">Haozhe Chen, Carl Vondrick, <strong>Chengzhi Mao</strong></div>
            <div class="meta">ICML 2024 — <a href="https://selfie.cs.columbia.edu">project</a></div>
          </div>
        </div>

        <div class="paper">
          <img class="thumb" src="assets/img/papers/raider.png" alt="Raidar thumbnail" />
          <div>
            <h3>Raidar: geneRative AI Detection viA Rewriting</h3>
            <div class="by"><strong>Chengzhi Mao</strong>, Carl Vondrick, Hao Wang, Junfeng Yang</div>
            <div class="meta">ICLR 2024 — <a href="https://arxiv.org/">paper</a></div>
          </div>
        </div>
      </div>

      <!-- 2023 -->
      <div class="year">2023</div>
      <div class="grid">
        <div class="paper">
          <img class="thumb" src="assets/img/papers/dr.png" alt="Doubly Right thumbnail" />
          <div>
            <h3>Doubly Right Object Recognition: A Why Prompt for Visual Rationales</h3>
            <div class="by"><strong>Chengzhi Mao</strong>, Revant Teotia, Amrutha Sundar, Sachit Menon, Junfeng Yang, Xin Wang, Carl Vondrick</div>
            <div class="meta">CVPR 2023 — <a href="#">arXiv</a> / <a href="#">dataset</a> / <a href="#">code</a></div>
            <p>Benchmark requiring models to be “doubly right”: correct labels and correct rationales, adapting CLIP with rationale transfer.</p>
          </div>
        </div>
      </div>

      <!-- 2022 -->
      <div class="year">2022</div>
      <div class="grid">
        <div class="paper">
          <img class="thumb" src="assets/img/papers/camo.gif" alt="Neural Voice Camouflage thumbnail" />
          <div>
            <h3>Real-Time Neural Voice Camouflage</h3>
            <div class="by">Mia Chiquier, <strong>Chengzhi Mao</strong>, Carl Vondrick</div>
            <div class="meta">ICLR 2022 (Oral) — <a href="#">arXiv</a> / <a href="#">code</a> / <a href="https://www.science.org/">Science</a> / <a href="#">talk</a></div>
            <p>Predictive adversarial attacks for streaming audio enable real-time neural voice camouflage.</p>
          </div>
        </div>

        <div class="paper">
          <img class="thumb" src="assets/img/papers/dr.png" alt="Discrete Representations thumbnail" />
          <div>
            <h3>Discrete Representations Strengthen Vision Transformer Robustness</h3>
            <div class="by"><strong>Chengzhi Mao</strong>, Lu Jiang, Mostafa Dehghani, Carl Vondrick, Rahul Sukthankar, Irfan Essa</div>
            <div class="meta">ICLR 2022 — <a href="#">arXiv</a> / <a href="#">code</a> / <a href="#">cite</a> / <a href="#">talk</a></div>
            <p>Quantized/discrete visual tokens improve ViT robustness under distribution shift and adversarial perturbations.</p>
          </div>
        </div>
      </div>

      <!-- 2021 -->
      <div class="year">2021</div>
      <div class="grid">
        <div class="paper">
          <img class="thumb" src="assets/img/papers/reverse_attack.png" alt="Adversarial Reversible thumbnail" />
          <div>
            <h3>Adversarial Attacks are Reversible with Natural Supervision</h3>
            <div class="by"><strong>Chengzhi Mao</strong>, Mia Chiquier, Hao Wang, Junfeng Yang, Carl Vondrick</div>
            <div class="meta">ICCV 2021 — <a href="#">arXiv</a> / <a href="#">code</a> / <a href="#">cite</a> / <a href="#">talk</a></div>
            <p>Adversarial attacks disrupt incidental structure; restoring natural priors reverses attacks for defense.</p>
          </div>
        </div>
      </div>

    </section>

    <section id="teaching">
      <h2>Teaching</h2>
      <ul class="teach">
        <li>Advanced Topics in Robust &amp; Causal Learning (Rutgers CS) — 2025–</li>
        <li>Guest lectures and seminars on LLM robustness and causal methods.</li>
      </ul>
    </section>

    <section id="contact">
      <h2>Contact</h2>
      <p>
        Department of Computer Science, Rutgers University<br/>
        Email: <a href="mailto:cm1838@scarletmail.rutgers.edu">cm1838@scarletmail.rutgers.edu</a>
      </p>
    </section>

    <footer>
      © <span id="y"></span> Chengzhi Mao · Simple, readable academic layout.
    </footer>
  </div>

  <script>document.getElementById('y').textContent = new Date().getFullYear()</script>
</body>
</html>
